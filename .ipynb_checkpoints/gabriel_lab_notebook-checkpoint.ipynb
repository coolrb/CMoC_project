{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Notebook for Final Project of Computational Models of Cognition\n",
    "## Authors: Gabe Brookman, James Yang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Oct. 10th:\n",
    "We came together to write our project proposal, which we both editted independently for the next day or two. The whole week leading up to this meeting, we had been sending papers back and forth and reading them, as well as brainstorming ideas for what topic to do our final project on, so this meeting was a natural culmination of our work thus far. We didn't begin implementation or do anything beyond background research, coming up with our idea, and writing our proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Oct 21st:\n",
    "We first updated our Github repo (https://github.com/formidify/CMoC_project): the read_me and todo files. We also created this Jupyter notebook to keep track of our progress. Then, we created a Python file to clean the Enron email dataset by including only the subjects and body (for now) of each email, as well as basic data cleaning such as removing symbols and common stop words (such as 'a', 'and' and 'an', etc.) from the email subject lines. We also got rid of reply and forward emails, as well as emails whose subjects include significant words that do not appear in the body (so that we can run extractive summarisation on that email).\n",
    "\n",
    "Later, on the network side of things, we settled on a framework (Keras) and a paradigm (functional) for our model. We began implementing a first draft for both of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes Oct 23rd:\n",
    "We continued to deal with cleaning the Enron directory of text files and compiling them into a readable CSV file. We cleared up a few commands, such as using `\" \".join(string.split())` instead of using the `strip()` command, etc. The results are outstanding, as we manage to get rid of all empty lines, including escape characters \"\\n\" and \"\\t\". In addition, we edited the code to make sure that cases where forward and subject info from another email are somehow visible. We managed to get rid of those lines and made sure that only relevant information are printed out. The number of how many usable emails we will then have for our training and testing is displayed above, after running everything. At most 3309 are valid can be used effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reply/forward or non-extractable emails: 11088\n",
      "Number of emails after basic cleaning, such as getting rid of ones with empty subject/body: 14397\n",
      "Number of actual emails used in the `final`-ish dataset: 3309\n"
     ]
    }
   ],
   "source": [
    "data_list = data_cleaning.read_text() # if the Enron txt directory is in the same directory\n",
    "csv = data_cleaning.create_csv(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said \"at most\" because the data cleaning is not over - there can be cases where the information needs further processing. For example, some emails may include the name and company and title of the sender. We do not want that information, and in the case that those information are all the email body has, we may have to exclude that email from our dataset. Same goes for numbers - the subject an email full of uninterpretable numbers can not be properly extracted by a program. Those are the conditions we will continue to explore and hopefully be able to deal with (numbers are not so much of a problem with tokenization problems such as `nltk`, but person information removal will be quite difficult)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Oct 28th:\n",
    "Today, we moved on to construct our model. We want the model to iterate over all of the words in the message body and return the probability that each of them are in the title. The network will take in one letter at a time. When it encounters whitespace, it will return the probability of the word just before that whitespace being in the title.\n",
    "\n",
    "We switched to PyTorch because it's much lower level than Keras, so it's easier to program PyTorch to only update on a per-word basis while still updating its hidden parameters on a per-letter basis.\n",
    "\n",
    "Do we want all of our hidden layers to be LSTM's, or just the first one? It's unclear, something that we'll have to look into. https://arxiv.org/pdf/1503.04069.pdf may have answers: \"Adding  full  gate  recurrence  (FGR)  did  not  significantly change  performance  on  TIMIT  or  IAM  Online,  but  led  to worse  results  on  the  JSB  Chorales  dataset.  Given  that  this variant greatly increases the number of parameters, we generally advise against using it.\" Well, that answers that question.\n",
    "\n",
    "We decided on 1 LSTM (27 to 100 nodes), followed by two linear layers (100 to 50 and 50 to 1 nodes respectively). We'll use ReLU for all of our activations besides the last one, which we'll use a sigmoid for (since we want it to be a probability).\n",
    "\n",
    "We now must decide how big a state (cell state, hidden state) we want to pass through between the different calls to our LSTM. This dude has some idea: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw. We'll just trust him for now, and stay nimble and experiment for this parameter.\n",
    "\n",
    "Upon further inspection, it seems that our hidden state and output state are the same, and so must have the same size. For some reason, our cell state must also have the same size as our hidden state. We don't quite understand why that is. Anyways, it should be fine (?) I guess.\n",
    "\n",
    "We completed our forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Oct 30th:\n",
    "Today, we edited the data cleaning script to get rid of all numbers, the symbol '$' and apostrophe, after we discussed that these symbols are not necessary, and can be potential distractions to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Oct 31st:\n",
    "Today, we edited our code to deal with different troubles that we were facing. In particular, we changed the apostrophe to be replaced by an empty string rather than being replaced by a space. Strangely, our dataset increased in size substatially after this change.\n",
    "\n",
    "On the model end, we began to specify in our code how we wanted our model to update on a per-character basis, but only compute output and gradient descend on a per-word basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we populated the modeltrainer.py with methods to process in the cleaned CSV file, read in each line, process them as tensors and pass them into the network train. One bug popped in the middle where we found empty subject/body, so we did another empty string check in data_cleaning.py before writing them in the CSV. In addition, we need some further cleaning in terms of stopwords removing. We can either do them in data_cleaning.py or modeltrainer.py, but James suggests that we should do it from modeltrainer because in many cases, stopwords removal should be considered more like a potential parameter to the quality of the network (also, having to call data_cleaning.py every time we change the stopwords list is simply inefficient).\n",
    "\n",
    "# Oct 31st part II:\n",
    "We met again in the evening to continue to work on developing our code. We established what format we'll use to input and output our data: each MESSAGE will be encoded as a list of WORDS, and there will be an associated key that contains the ground truth for whether or not those words are in the subject line. Each WORD will be a list of tensors representing characters in that word. The final character in every word is a space.\n",
    "\n",
    "Our model will take in input one character at a time, but only present its output on a per-word basis.\n",
    "\n",
    "James worked to create functions to convert the existing data to a better format, and then started a function to train the model from start to finish (on a high layer of abstraction). Gabriel contributed by writing the functions that train the model on a given message (low layer of abstraction). To do this, Gabriel had to select a loss function -- we settled on binary cross entropy -- and then write the rest of the code. For both of us, the bulk of the code is stuff that we had already written or used from problem set 6, slightly adapted because of the new context.\n",
    "\n",
    "Next time or before, we're thinking of writing code to allow the model to test on test data and predict the subject line of a novel email. After that, we'll be able to start training the model, by which I mean debugging since PyTorch is so finicky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nov. 7th\n",
    "We met in the CMC for three hours to try and finally get our model running. On both sides of our code, we had lots of errors that we fixed cooperatively. The most distressing two were the following: because our output is between 0 and 1, we opted to switch to a BCE (binary cross entropy) loss instead of our classic NLLLoss. In addition, we had to remove the hidden layers from consideration for our passes forward through time, instead only incorporating the hidden (between LSTM and output) for our guesses for each word. This means that we have fewer parameters to work with, and only use a single LSTM for regulating our memory rather than a series of layers like I had hoped. This may spell doom for our project, but hopefully not.\n",
    "\n",
    "After the errors were fixed, we set to work training our model. After about 80 minutes of training, the model showed no progress. To rectify this, we plan to A. reduce the number of nodes in the LSTM and B. run the model on a Carleton server as opposed to James's laptop. As our LSTM currently has a 400 length hidden vector and a 400 length cell vector, we may be able to dip lower without destroying our performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nov. 8th\n",
    "We met again in the morning. We began writing up our paper, and meanwhile ran our model with a few tweeks. We lowered the number of parameters a few times, but it's unclear what our final parameter count will wind up being. We may try many different numbers and see what happens, and even compare our results in the paper.\n",
    "\n",
    "We also editted our data a bit. There are several emails with lengths of thousands of words in our dataset. These emails take a massive quantity of time for our model to learn from, yet the model doesn't learn any more than a normal email. What's more, the model may learn that subject-words are very uncommon because almost none of the 10,000 words in the mega-emails appear in the title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nov. 9th\n",
    "We copied our model several times with different numbers of nodes, and ran them all. The biggest of them (400/200) takes about 3-4 hours per epoch to train. The next biggest (100/50) takes just 47-48 minutes. We set several different sizes (both of the ones mentioned, and also 4,2 and 20,10) to train overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nov. 10th\n",
    "We looked at our training results. For the low numbers of parameters, they were incredibly lackluster. Our model learned the base rates that a word is included and simply predicted this for every word, constantly. Awful. For our models with higher numbers of parameters, we have slightly more hopeful results, yet it seems that in the ten epochs that we afforded for them to run, they haven't converged.\n",
    "\n",
    "Also troubling was the way stopwords were handled. First of all, we didn't have enough of them. There were some words, like \"am\" that made it through. Second of all, with stopwords missing, many of the email messages that we used as training data were borderline indecipherable even to our well-trained human eyes. To cope with these problems, we expanded the stopwords list to include more of the common words, and furthermore we modified our model so that it intakes stopwords so that it can use them to inform how it predicts other words, but it doesn't use the stopwords themselves to update training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
